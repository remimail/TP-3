{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yfinance\n",
    "!pip install git+https://github.com/dcajasn/Riskfolio-Lib.git\n",
    "!pip install PyPortfolioOpt\n",
    "\n",
    "#Faut changer notre version de numpy et pandas, pour l'avant dernière\n",
    "!pip install Numpy==1.23.5\n",
    "!pip install pandas==1.3.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  11 of 11 completed\n"
     ]
    }
   ],
   "source": [
    "run data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  11 of 11 completed\n"
     ]
    }
   ],
   "source": [
    "# Tickers list\n",
    "# We can add and delete any ticker from the list to get desired ticker live data\n",
    "ticker_list=['ESGU', 'EAGG', 'ESGE', 'ESML', 'SUSB', 'ESGD', 'SHY', 'SUSA' , 'GOVT', 'MBB', 'SUSC']\n",
    "\n",
    "etf_monthly_rets = getData(ticker_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EAGG</th>\n",
       "      <th>ESGD</th>\n",
       "      <th>ESGE</th>\n",
       "      <th>ESGU</th>\n",
       "      <th>ESML</th>\n",
       "      <th>GOVT</th>\n",
       "      <th>MBB</th>\n",
       "      <th>SHY</th>\n",
       "      <th>SUSA</th>\n",
       "      <th>SUSB</th>\n",
       "      <th>SUSC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2002-07-31</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-08-31</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004307</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-09-30</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-10-31</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-11-30</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.005477</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-31</th>\n",
       "      <td>-0.002968</td>\n",
       "      <td>0.025093</td>\n",
       "      <td>0.062915</td>\n",
       "      <td>0.034886</td>\n",
       "      <td>0.052083</td>\n",
       "      <td>-0.005240</td>\n",
       "      <td>-0.003753</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.037057</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>-0.002199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-31</th>\n",
       "      <td>-0.008718</td>\n",
       "      <td>-0.039192</td>\n",
       "      <td>-0.070791</td>\n",
       "      <td>-0.016359</td>\n",
       "      <td>-0.039069</td>\n",
       "      <td>-0.007463</td>\n",
       "      <td>-0.010977</td>\n",
       "      <td>0.001479</td>\n",
       "      <td>-0.016579</td>\n",
       "      <td>-0.001250</td>\n",
       "      <td>-0.010577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-30</th>\n",
       "      <td>-0.028314</td>\n",
       "      <td>-0.037589</td>\n",
       "      <td>-0.030730</td>\n",
       "      <td>-0.053422</td>\n",
       "      <td>-0.062935</td>\n",
       "      <td>-0.025210</td>\n",
       "      <td>-0.033732</td>\n",
       "      <td>-0.003323</td>\n",
       "      <td>-0.057906</td>\n",
       "      <td>-0.008344</td>\n",
       "      <td>-0.030735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-31</th>\n",
       "      <td>-0.018322</td>\n",
       "      <td>-0.031824</td>\n",
       "      <td>-0.030053</td>\n",
       "      <td>-0.023959</td>\n",
       "      <td>-0.065379</td>\n",
       "      <td>-0.014065</td>\n",
       "      <td>-0.024550</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>-0.035345</td>\n",
       "      <td>-0.002945</td>\n",
       "      <td>-0.022518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-30</th>\n",
       "      <td>0.032157</td>\n",
       "      <td>0.074257</td>\n",
       "      <td>0.078652</td>\n",
       "      <td>0.086843</td>\n",
       "      <td>0.075676</td>\n",
       "      <td>0.026231</td>\n",
       "      <td>0.042253</td>\n",
       "      <td>0.003826</td>\n",
       "      <td>0.089642</td>\n",
       "      <td>0.013080</td>\n",
       "      <td>0.043724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>257 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                EAGG      ESGD      ESGE      ESGU      ESML      GOVT  \\\n",
       "Date                                                                     \n",
       "2002-07-31  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2002-08-31  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2002-09-30  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2002-10-31  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2002-11-30  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2023-07-31 -0.002968  0.025093  0.062915  0.034886  0.052083 -0.005240   \n",
       "2023-08-31 -0.008718 -0.039192 -0.070791 -0.016359 -0.039069 -0.007463   \n",
       "2023-09-30 -0.028314 -0.037589 -0.030730 -0.053422 -0.062935 -0.025210   \n",
       "2023-10-31 -0.018322 -0.031824 -0.030053 -0.023959 -0.065379 -0.014065   \n",
       "2023-11-30  0.032157  0.074257  0.078652  0.086843  0.075676  0.026231   \n",
       "\n",
       "                 MBB       SHY      SUSA      SUSB      SUSC  \n",
       "Date                                                          \n",
       "2002-07-31  0.000000  0.003086  0.000000  0.000000  0.000000  \n",
       "2002-08-31  0.000000  0.004307  0.000000  0.000000  0.000000  \n",
       "2002-09-30  0.000000  0.005759  0.000000  0.000000  0.000000  \n",
       "2002-10-31  0.000000  0.000975  0.000000  0.000000  0.000000  \n",
       "2002-11-30  0.000000 -0.005477  0.000000  0.000000  0.000000  \n",
       "...              ...       ...       ...       ...       ...  \n",
       "2023-07-31 -0.003753  0.000493  0.037057  0.003764 -0.002199  \n",
       "2023-08-31 -0.010977  0.001479 -0.016579 -0.001250 -0.010577  \n",
       "2023-09-30 -0.033732 -0.003323 -0.057906 -0.008344 -0.030735  \n",
       "2023-10-31 -0.024550  0.000741 -0.035345 -0.002945 -0.022518  \n",
       "2023-11-30  0.042253  0.003826  0.089642  0.013080  0.043724  \n",
       "\n",
       "[257 rows x 11 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etf_monthly_rets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sortir les premieres dates pour chaque ETF et son Index benchmark associe: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index_rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_index_rets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Verifier les dates des df pour etf et index associe\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ticker_list: \n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mETF \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Benchmark Index initiation date: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_index_rets[i]\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mETF \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Benchmark Index initiation date: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00metf_monthly_rets[i]\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_index_rets' is not defined"
     ]
    }
   ],
   "source": [
    "# Verifier les dates des df pour etf et index associe\n",
    "for i in ticker_list: \n",
    "    print(f'ETF {i} Benchmark Index initiation date: {df_index_rets[i].head(1).index}')\n",
    "    print(f'ETF {i} Benchmark Index initiation date: {etf_monthly_rets[i].head(1).index}')\n",
    "    print('', sep='\\n')\n",
    "df_index_rets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series of the Indexes and the ETFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#====================================================================================\n",
    "# Grid of time seiries of the Indexes monthly returns \n",
    "\n",
    "# Plotting in a 3 by 4 grid\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 9), sharex=True)\n",
    "\n",
    "# Flatten the 2D array of subplots for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each column in a separate subplot\n",
    "for i, column in enumerate(df_imputed_index.columns):\n",
    "    df_imputed_index[column].plot(ax=axes[i], legend=True, grid=True)\n",
    "    axes[i].set_title(column)\n",
    "    axes[i].set_xlabel('Date')  # Set x-axis label on each subplot\n",
    "    ax.set_ylabel('Return')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "fig.suptitle('Benchmark Indexes Time Series', fontsize=16).set_y(1.01)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Grid of time seiries of the ETFs monthly returns \n",
    "\n",
    "# Determine the number of subplots based on the number of columns\n",
    "num_subplots = len(etf_monthly_rets.columns)\n",
    "\n",
    "# Calculate the number of rows and columns for the subplot grid\n",
    "num_rows = int(np.ceil(num_subplots / 3))\n",
    "num_cols = min(num_subplots, 3)\n",
    "\n",
    "# Plotting in a dynamic grid\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(12, 9), sharex=True)\n",
    "\n",
    "# Flatten the 2D array of subplots for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each column in a separate subplot\n",
    "for i, column in enumerate(etf_monthly_rets.columns):\n",
    "    etf_monthly_rets[column].plot(ax=axes[i], legend=True, grid=True)\n",
    "    axes[i].set_title(column)\n",
    "    axes[i].set_xlabel('Date')\n",
    "    axes[i].set_ylabel('Return')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "fig.suptitle('ETFs Time Series', fontsize=16).set_y(1.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking error between ETF's and associated benchmark Index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TE_fct(df_index_rets, etf_monthly_rets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing data to index dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TE_fct' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m knn: \n\u001b[0;32m      4\u001b[0m     imputer \u001b[38;5;241m=\u001b[39m KNNImputer(n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)  \u001b[38;5;66;03m# You can adjust the number of neighbors as needed\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     TE_fct(pd\u001b[38;5;241m.\u001b[39mDataFrame(imputer\u001b[38;5;241m.\u001b[39mfit_transform(df_index_rets), columns\u001b[38;5;241m=\u001b[39mdf_index_rets\u001b[38;5;241m.\u001b[39mcolumns, index\u001b[38;5;241m=\u001b[39mdf_index_rets\u001b[38;5;241m.\u001b[39mindex),\n\u001b[0;32m      6\u001b[0m            pd\u001b[38;5;241m.\u001b[39mDataFrame(imputer\u001b[38;5;241m.\u001b[39mfit_transform(etf_monthly_rets), columns\u001b[38;5;241m=\u001b[39metf_monthly_rets\u001b[38;5;241m.\u001b[39mcolumns, index\u001b[38;5;241m=\u001b[39metf_monthly_rets\u001b[38;5;241m.\u001b[39mindex))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TE_fct' is not defined"
     ]
    }
   ],
   "source": [
    "knn = [5,10,25,50]\n",
    "\n",
    "for k in knn: \n",
    "    imputer = KNNImputer(n_neighbors=50)  # You can adjust the number of neighbors as needed\n",
    "    TE_fct(pd.DataFrame(imputer.fit_transform(df_index_rets), columns=df_index_rets.columns, index=df_index_rets.index),\n",
    "           pd.DataFrame(imputer.fit_transform(etf_monthly_rets), columns=etf_monthly_rets.columns, index=etf_monthly_rets.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\# Create a KNNImputer instance\n",
    "imputer = KNNImputer(n_neighbors=50)  # You can adjust the number of neighbors as needed\n",
    "# Fit the imputer on the data and transform the DataFrame\n",
    "df_imputed_index = pd.DataFrame(imputer.fit_transform(df_index_rets), columns=df_index_rets.columns, index=df_index_rets.index)\n",
    "df_imputed_etf = pd.DataFrame(imputer.fit_transform(etf_monthly_rets), columns=etf_monthly_rets.columns, index=etf_monthly_rets.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------- Importation des donnees from FRED Website ------------------------------------------------\n",
    "# Import Features via FRED API \n",
    "\n",
    "# Features lists\n",
    "features_list = ['BAMLCC0A0CMTRIV', 'BAMLC0A4CBBB', 'BAMLC0A3CA', 'HQMCB10YR', 'FEDFUNDS', 'T10Y2Y', 'TB3SMFFM',\n",
    "                 'T5YFF', 'T1YFF', 'DLTIIT', 'NASDAQCOM', 'WILL5000PR', 'WILLLRGCAP', 'WILLSMLCAP', 'WILLLRGCAPGR', \n",
    "                 'WILLLRGCAPVAL', 'WILLMIDCAP', 'MSPUS', 'CCSA', 'BOPGSTB', 'VIXCLS', 'USSLIND', 'USALOLITONOSTSAM', 'UNRATE', \n",
    "                 'STICKCPIM157SFRBATL', 'EMVMACROBUS', 'MORTGAGE30US',  'WILLRESIPR']\n",
    "\n",
    "\n",
    "df_features = pd.DataFrame(columns=features_list )\n",
    "\n",
    "for i in features_list:\n",
    "    try:\n",
    "        data= import_fred(i)\n",
    "        df_features[i] = data.resample('M').last()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {i}: {e}\")\n",
    "        \n",
    "# Create a KNNImputer instance\n",
    "imputer = KNNImputer(n_neighbors=5)  # You can adjust the number of neighbors as needed\n",
    "# Fit the imputer on the data and transform the DataFrame\n",
    "df_features = pd.DataFrame(imputer.fit_transform(df_features), columns=df_features.columns, index=df_features.index)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_imputed_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Clustermap of correlation of benchmark indexes returns\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m imputed_index_corr \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(df_imputed_index)\u001b[38;5;241m.\u001b[39mcorr()\n\u001b[0;32m      4\u001b[0m sns\u001b[38;5;241m.\u001b[39mclustermap(imputed_index_corr, annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, square\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39msuptitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClustermap of the correlation between returns of the Benchmark Indexes\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mset_y(\u001b[38;5;241m1.01\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_imputed_index' is not defined"
     ]
    }
   ],
   "source": [
    "# Clustermap of correlation of benchmark indexes returns\n",
    "imputed_index_corr = pd.DataFrame(df_imputed_index).corr()\n",
    "\n",
    "sns.clustermap(imputed_index_corr, annot=True, square=True)\n",
    "plt.suptitle('Clustermap of the correlation between returns of the Benchmark Indexes').set_y(1.01)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing features \n",
    "\n",
    "* equity indexes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning methods to predict next month return\n",
    "* Random Forest \n",
    "* Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustermap of correlation of benchmark indexes returns\n",
    "# Compute corr matrix\n",
    "imputed_index_corr = pd.DataFrame(df_imputed_index).corr()\n",
    "\n",
    "sns.clustermap(imputed_index_corr, annot=True, square=True)\n",
    "plt.suptitle('Clustermap of the correlation between returns of the Benchmark Indexes').set_y(1.01)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustermap of the correlation between the features\n",
    "x_corr = pd.DataFrame(x).corr()\n",
    "sns.clustermap(x_corr, annot=True)\n",
    "plt.suptitle('Clustermap of the correlation between features').set_y(1.01)\n",
    "\n",
    "plt.show();      # we can see that the equity market indexes are strongly correlated \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning \n",
    "len(df_features)\n",
    "len(df_imputed_index)\n",
    "\n",
    "df_features = df_features.loc[df_imputed_index.head(1).index[0]:df_imputed_index.tail(1).index[0]]\n",
    "\n",
    "merged_df = pd.merge(df_features, df_imputed_index, left_index=True, right_index=True)\n",
    "\n",
    "# Specify columns to lag\n",
    "columns_to_lag = features_list\n",
    "\n",
    "# Number of periods to lag\n",
    "lag_periods = 1\n",
    "\n",
    "# Lag the specified columns\n",
    "merged_df[columns_to_lag] = merged_df[columns_to_lag].shift(lag_periods)\n",
    "merged_df.dropna(inplace=True, axis=0)\n",
    "\n",
    "x = merged_df[features_list]\n",
    "y = merged_df[ticker_list] \n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Assuming x_scaled_corr is your correlation matrix\n",
    "x_scaled_corr = pd.DataFrame(x_scaled).corr()\n",
    "x_scaled_corr.index = features_list\n",
    "x_scaled_corr.columns = features_list\n",
    "\n",
    "# Create a clustermap\n",
    "clustermap = sns.clustermap(x_scaled_corr, annot=True, figsize=(15, 15),  annot_kws={\"fontsize\": 8})\n",
    "# Add a title to the clustermap\n",
    "plt.suptitle('Clustermap of the correlation between standardized features').set_y(1.01)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice some strong correlations levels are observable among the historical features values. This makes sense since we do expect indeed that interest rate measure move together. This is also true for total market equity indexes used such as S&P500, Wilshire 1000, Wildshire total market, etc.  The following table describe the feature associated with the tick presented in the clustermap. \n",
    "\n",
    "**Bonds**\n",
    " \n",
    "- **BAMLCC0A0CMTRIV**: ICE BofA US Corporate Index Total Return Index Value <br>\n",
    "- **BAMLC0A4CBBB**: ICE BofA BBB US Corporate Index Option-Adjusted Spread  <br>\n",
    "- **BAMLC0A3CA**: ICE BofA Single-A US Corporate Index Option-Adjusted Spread  <br>\n",
    "- **HQMCB10YR**: 10-Year High Quality Market (HQM) Corporate Bond Spot Rate <br>\n",
    "- **FEDFUNDS**: Federal Funds Effective Rate <br>\n",
    "- **T10Y2Y**: 10-Year Treasury Constant Maturity Minus 2-Year Treasury Constant Maturity <br>\n",
    "- **TB3SMFFM**: 3-Month Treasury Bill Minus Federal Funds Rate <br>\n",
    "- **T5YFF**: 5-Year Treasury Constant Maturity Minus Federal Funds Rate <br>\n",
    "- **T1YFF**: 1-Year Treasury Constant Maturity Minus Federal Funds Rate <br>\n",
    "- **DLTIIT**: Treasury Long-Term Average (Over 10 Years), Inflation-Indexed <br>\n",
    "\n",
    "\n",
    "**Equity Indexes**\n",
    "- **NASDAQCOM**: NASDAQ Composite Index<br>\n",
    "- **WILL5000PR**: Wilshire 5000 Price Index<br>\n",
    "- **WILLLRGCAP**: Wilshire US Large-Cap Total Market<br>\n",
    "- **WILLSMLCAP**: Wilshire US Small-Cap Total Market Index<br>\n",
    "- **WILLLRGCAPGR**: Wilshire US Large-Cap Growth Total Market Index<br>\n",
    "- **WILLLRGCAPVAL**: Wilshire US Large-Cap Value Total Market Index<br>\n",
    "- **WILLMIDCAP**: Wilshire US Mid-Cap Total Market Index<br>\n",
    "- **SBPREUE**: S&P Europe LargeMidCap <br>\n",
    "- **MXEUMC**: MSCI Europe Mid Cap Index (USD)<br>\n",
    "- **MXEULC**: MSCI Europe Large Cap Index (USD)<br>\n",
    "- **SPAXLCUP**: S&P Pan Asia Ex-JP, AU, NZ, IN LargeCap Index<br>\n",
    "- **SBPRAPU**: S&P Asia Pacific LargeMidCap<br>\n",
    "- **MEMMG**: Morningstar Emerging Markets<br>\n",
    "- **MXEF**: MSCI Emerging Markets Index<br>\n",
    "- **MXEFLC**: MSCI Emerging Markets Large Cap Index (USD)<br>\n",
    "- **MXEFMC**: MSCI Emerging Markets Mid Cap Index (USD)<br>\n",
    "- **SML**: S&P SmallCap 600<br>\n",
    "\n",
    "\n",
    "**Other**\n",
    "- **MSPUS**: Median Sales Price of Houses Sold for the United States <br>\n",
    "- **CCSA**: Continued Claims (Insured Unemployment) <br>\n",
    "- **BOPGSTB**: Trade Balance: Goods and Services, Balance of Payments Basis<br>\n",
    "- **VIXCLS**: CBOE Volatility Index: VIX<br>\n",
    "- **USSLIND**: Leading Index for the United States<br>\n",
    "- **USALOLITONOSTSAM**: Leading Indicators OECD: Leading Indicators: Composite Leading Indicator: Normalised for United States<br>\n",
    "- **UNRATE**: Unemployment Rate<br>\n",
    "- **STICKCPIM157SFRBATL**: Sticky Price Consumer Price Index less Food and Energy       <br>\n",
    "- **EMVMACROBUS**: Equity Market Volatility Tracker: Macroeconomic News and Outlook: Business Investment And Sentiment<br>\n",
    "- **MORTGAGE30US**: 30-Year Fixed Rate Mortgage Average in the United States<br>\n",
    "- **WILLRESIPR**: Wilshire US Real Estate Securities Price Index (Wilshire US RESI)<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Random Forest \n",
    "\n",
    "# Initialize an empty DataFrame to store predicted returns\n",
    "predicted_returns_df = pd.DataFrame(index=merged_df.index)\n",
    "\n",
    "# Number of folds for cross-validation\n",
    "num_folds = 5\n",
    "\n",
    "# Create a 4x3 grid for subplots\n",
    "num_rows = 4\n",
    "num_cols = 3\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(15, 10), sharex=True)\n",
    "\n",
    "# Flatten the 2D array of subplots for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate over each index\n",
    "for i, ticker in enumerate(ticker_list):\n",
    "    # Extract features and target variable for the current stock\n",
    "    x = merged_df[features_list]\n",
    "    y = merged_df[ticker]\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    x_scaled = scaler.fit_transform(x)\n",
    "    \n",
    "    # K-fold cross-validation\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Initialize model\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # Lists to store evaluation metrics\n",
    "    mae_list = []\n",
    "    \n",
    "    # Lists to store actual and predicted returns for plotting\n",
    "    actual_returns_list = []\n",
    "    predicted_returns_list = []\n",
    "    \n",
    "    # Iterate over folds\n",
    "    for train_index, test_index in kf.split(x_scaled):\n",
    "        X_train, X_test = x_scaled[train_index], x_scaled[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Model Training\n",
    "        rf_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Model Prediction\n",
    "        y_pred = rf_model.predict(X_test)\n",
    "        \n",
    "        # Evaluate Model Performance\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mae_list.append(mae)\n",
    "        \n",
    "        # Store actual and predicted returns for plotting\n",
    "        actual_returns_list.extend(y_test)\n",
    "        predicted_returns_list.extend(y_pred)\n",
    "    \n",
    "    # Calculate average MAE across folds for the current stock\n",
    "    average_mae = sum(mae_list) / len(mae_list)\n",
    "    print(f'Average Mean Absolute Error for {ticker} across {num_folds}-fold cross-validation: {average_mae}')\n",
    "    \n",
    "    # Predict returns for the entire period\n",
    "    predicted_returns = rf_model.predict(x_scaled)\n",
    "    \n",
    "    # Store predicted returns in the DataFrame\n",
    "    predicted_returns_df[ticker] = predicted_returns\n",
    "    \n",
    "    # Plot the actual and predicted time series\n",
    "    axes[i].plot(merged_df.index, y, label=f'Actual - {ticker}', color='blue')\n",
    "    axes[i].plot(merged_df.index, predicted_returns, label=f'Predicted - {ticker}', linestyle='--', color='orange')\n",
    "    axes[i].set_title(f'Actual vs Predicted Returns for {ticker}')\n",
    "    axes[i].set_xlabel('Date')\n",
    "    axes[i].set_ylabel('Returns')\n",
    "    axes[i].legend()\n",
    "\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the DataFrame with predicted returns for each stock\n",
    "print(\"\\nPredicted Returns DataFrame:\")\n",
    "print(predicted_returns_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  Decision Tree \n",
    "# Initialize an empty DataFrame to store predicted returns for Decision Tree\n",
    "predicted_returns_df_dt = pd.DataFrame(index=merged_df.index)\n",
    "\n",
    "# Create a 4x3 grid for subplots for Decision Tree\n",
    "fig_dt, axes_dt = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(15, 10), sharex=True)\n",
    "axes_dt = axes_dt.flatten()\n",
    "\n",
    "for i, ticker in enumerate(ticker_list):\n",
    "    # Initialize Decision Tree model\n",
    "    dt_model = DecisionTreeRegressor(random_state=42)\n",
    "    \n",
    "    # Lists to store actual and predicted returns for Decision Tree\n",
    "    actual_returns_list_dt = []\n",
    "    predicted_returns_list_dt = []\n",
    "    \n",
    "    # K-fold cross-validation for Decision Tree\n",
    "    for train_index, test_index in kf.split(x_scaled):\n",
    "        X_train, X_test = x_scaled[train_index], x_scaled[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Model Training for Decision Tree\n",
    "        dt_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Model Prediction for Decision Tree\n",
    "        y_pred_dt = dt_model.predict(X_test)\n",
    "        \n",
    "        # Store actual and predicted returns for Decision Tree\n",
    "        actual_returns_list_dt. extend(y_test)\n",
    "        predicted_returns_list_dt.extend(y_pred_dt)\n",
    "    \n",
    "    # Predict returns for the entire period using Decision Tree\n",
    "    predicted_returns_dt = dt_model.predict(x_scaled)\n",
    "    \n",
    "    # Store predicted returns in the DataFrame for Decision Tree\n",
    "    predicted_returns_df_dt[ticker] = predicted_returns_dt\n",
    "    \n",
    "    # Plot the actual and predicted time series for Decision Tree\n",
    "    axes_dt[i].plot(merged_df.index, y, label=f'Actual - {ticker}', color='blue')\n",
    "    axes_dt[i].plot(merged_df.index, predicted_returns_dt, label=f'Predicted (DT) - {ticker}', linestyle='--', color='yellow')\n",
    "    axes_dt[i].set_title(f'Actual vs Predicted Returns for {ticker} (Decision Tree)')\n",
    "    axes_dt[i].set_xlabel('Date')\n",
    "    axes_dt[i].set_ylabel('Returns')\n",
    "    axes_dt[i].legend()\n",
    "\n",
    "# Adjust layout for Decision Tree\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the DataFrame with predicted returns for each stock using Decision Tree\n",
    "print(\"\\nPredicted Returns DataFrame (Decision Tree):\")\n",
    "print(predicted_returns_df_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monthly returns predictions \n",
    "\n",
    "The following Dataframes containt the monthly return predictions made by the respective machine learning method based on the previous month feature values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Tree predicted Monthly returns for each Index (ETF): \n",
    "predicted_returns_df_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest predicted Monthly returns for each Index (ETF): \n",
    "predicted_returns_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remi's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Annualized variance of returns\n",
    "np.sqrt(rets.var()*252)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk parity relaxed optim based on historical rets and cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the linear constraint based on the ESG rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from riskfolio import factors_constraints\n",
    "\n",
    "# Your DataFrame\n",
    "ESG_constraint_data = {\n",
    "    'ESG Fund Rating': [6.6, 8.6, 7.1, 7.3, 6.5, 5.7, 6, 5.7, 8.2, 7.9, 7.6],\n",
    "}\n",
    "\n",
    "ESG_constraint = pd.DataFrame(ESG_constraint_data)\n",
    "\n",
    "# Define the constraints based on your DataFrame\n",
    "constraints = pd.DataFrame({\n",
    "    'Disabled': [False],\n",
    "    'Factor': ['ESG Fund Rating'],\n",
    "    'Sign': ['>='],\n",
    "    'Value': [7],\n",
    "    'Relative Factor': '',\n",
    "})\n",
    "\n",
    "# Create the factors constraints matrices C and D\n",
    "C, D = factors_constraints(constraints, loadings=ESG_constraint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the portfolio object nea== minimal number of assets to include\n",
    "port = rp.Portfolio(returns=rets,ainequality = C, binequality = D, nea = 6)\n",
    "\n",
    "# Select method and estimate input parameters:\n",
    "\n",
    "method_mu='hist' # Method to estimate expected returns based on historical data.\n",
    "method_cov='hist' # Method to estimate covariance matrix based on historical data.\n",
    "\n",
    "port.assets_stats(method_mu=method_mu, method_cov=method_cov, d=0.94)\n",
    "\n",
    "# Estimate optimal portfolio:\n",
    "\n",
    "model = 'Classic' # Could be Classic (historical) or FM (Factor Model)\n",
    "rm = 'MV' # Risk measure used, this time will be variance\n",
    "obj = 'MinRisk' # Objective function, could be MinRisk, MaxRet, Utility or Sharpe\n",
    "hist = True # Use historical scenarios for risk measures that depend on scenarios\n",
    "rf = 0 # Risk free rate\n",
    "b = None # Risk contribution constraints vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum Variance Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model='Classic' # Could be Classic (historical), BL (Black Litterman) or FM (Factor Model)\n",
    "rm = 'MV' # Risk measure used, this time will be variance\n",
    "obj = 'MinRisk' # Objective function, could be MinRisk, MaxRet, Utility or Sharpe\n",
    "hist = True # Use historical scenarios for risk measures that depend on scenarios\n",
    "rf = 0 # Risk free rate\n",
    "l = 0 # Risk aversion factor, only useful when obj is 'Utility'\n",
    "\n",
    "w = port.optimization(model=model, rm=rm, obj=obj, rf=rf, l=l, hist=hist)\n",
    "\n",
    "display(w.T)\n",
    "\n",
    "# Plotting the composition of the portfolio\n",
    "\n",
    "ax = rp.plot_pie(w=w, title='Minnimum Variance Portfolio', others=0.05, nrow=25, cmap = \"tab20\",\n",
    "                 height=6, width=10, ax=None)\n",
    "\n",
    "\n",
    "w_weights = w['weights'].values.reshape(1, -1)\n",
    "esg_ratings = ESG_constraint['ESG Fund Rating'].values.reshape(-1, 1)\n",
    "\n",
    "ESG_Rating_portfolio = w_weights.dot(esg_ratings).item()\n",
    "print(f'The weighted average ESG rating of this portfolio is {ESG_Rating_portfolio}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Parity Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'A' # Could be A, B or C\n",
    "\n",
    "w_rrp_a = port.rrp_optimization(model=model, version=version, l=l, b= None, hist=hist)\n",
    "\n",
    "display(w_rrp_a.T)\n",
    "# Plotting the composition of the portfolio\n",
    "\n",
    "ax = rp.plot_pie(w=w_rrp_a, title='Relaxed Risk Parity A', others=0.05, nrow=25, cmap = \"tab20\",\n",
    "                 height=6, width=10, ax=None)\n",
    "\n",
    "\n",
    "w_rrp_a_weights = w_rrp_a['weights'].values.reshape(1, -1)\n",
    "esg_ratings = ESG_constraint['ESG Fund Rating'].values.reshape(-1, 1)\n",
    "\n",
    "ESG_Rating_portfolio = w_rrp_a_weights.dot(esg_ratings).item()\n",
    "print(f'The weighted average ESG rating of this portfolio is {ESG_Rating_portfolio}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "# Plotting the risk composition of the portfolio\n",
    "ax = rp.plot_risk_con(w_rrp_a, cov=port.cov, returns=port.returns, rm=rm, rf=0, alpha=0.05,\n",
    "                      color=\"tab:blue\", height=6, width=10, ax=ax)\n",
    "\n",
    "# Plotting equal risk contribution line\n",
    "a1 = rp.Sharpe_Risk(w_rrp_a, cov=port.cov, returns=port.returns, rm=rm, rf=0, alpha=0.05)\n",
    "ax.axhline(y=a1/len(assets) * 252**0.5, color='r', linestyle='-')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Views on Black Litterman Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_classes = {'Assets':[\"ESGU\", \"EAGG\", \"ESGE\", \"ESML\", \"SUSB\", \"ESGD\", \"SHY\", \"SUSA\", \"GOVT\", \"MBB\", \"SUSC\"]}\n",
    "\n",
    "asset_classes = pd.DataFrame(asset_classes)\n",
    "asset_classes = asset_classes.sort_values(by=['Assets'])\n",
    "\n",
    "views = {'Disabled': [False, False, False],\n",
    "         'Type': ['Assets', 'Assets', 'Assets'],\n",
    "         'Set': ['', '', ''],\n",
    "         'Position': ['SHY', 'ESGE', 'SUSB'],\n",
    "         'Sign': ['<=', '>=', '>='],\n",
    "         'Return': [0.08, 0.5, 0], # Annual terms \n",
    "         'Type Relative': [ '', '', ''],\n",
    "         'Relative Set': [ '', '', ''],\n",
    "         'Relative': ['', '', '']\n",
    "}\n",
    "\n",
    "views = pd.DataFrame(views)\n",
    "\n",
    "display(views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P,Q = rp.assets_views(views, asset_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Black Litterman optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Black Litterman inputs:\n",
    "\n",
    "port.blacklitterman_stats(P, Q/252, rf=rf, w=w, delta=None, eq=True)\n",
    "\n",
    "# Estimate optimal portfolio:\n",
    "\n",
    "model='BL'# Black Litterman\n",
    "rm = 'MV' # Risk measure used, this time will be variance\n",
    "obj = 'Sharpe' # Objective function, could be MinRisk, MaxRet, Utility or Sharpe\n",
    "hist = False # Use historical scenarios for risk measures that depend on scenarios\n",
    "\n",
    "w_bl = port.optimization(model=model, rm=rm, obj=obj, rf=rf, l=l, hist=hist)\n",
    "\n",
    "display(w_bl.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the composition of the portfolio\n",
    "\n",
    "ax = rp.plot_pie(w=w_bl, title='Sharpe Black Litterman', others=0.05, nrow=25,\n",
    "                 cmap = \"tab20\", height=6, width=10, ax=None)\n",
    "\n",
    "w_bl_weights = w_bl['weights'].values.reshape(1, -1)\n",
    "\n",
    "ESG_Rating_BLportfolio = w_bl_weights.dot(esg_ratings).item()\n",
    "print(f'The weighted average ESG rating of this BL portfolio is {ESG_Rating_BLportfolio}')               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = 100 # Number of points of the frontier\n",
    "\n",
    "frontier = port.efficient_frontier(model=model, rm=rm, points=points, rf=rf, hist=hist)\n",
    "\n",
    "display(frontier.T.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the efficient frontier\n",
    "\n",
    "label = 'Max Risk Adjusted Return Portfolio' # Title of point\n",
    "mu = port.mu_bl # Expected returns of Black Litterman model\n",
    "cov = port.cov_bl # Covariance matrix of Black Litterman model\n",
    "returns = port.returns # Returns of the assets\n",
    "\n",
    "ax = rp.plot_frontier(w_frontier=frontier, mu=mu, cov=cov, returns=returns, rm=rm,\n",
    "                      rf=rf, alpha=0.05, cmap='viridis', w=w_bl, label=label,\n",
    "                      marker='*', s=16, c='r', height=6, width=10, ax=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting efficient frontier composition\n",
    "\n",
    "ax = rp.plot_frontier_area(w_frontier=frontier, cmap=\"tab20\", height=6, width=10, ax=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk Measures available:\n",
    "#\n",
    "# 'MV': Standard Deviation.\n",
    "# 'MAD': Mean Absolute Deviation.\n",
    "# 'MSV': Semi Standard Deviation.\n",
    "# 'FLPM': First Lower Partial Moment (Omega Ratio).\n",
    "# 'SLPM': Second Lower Partial Moment (Sortino Ratio).\n",
    "# 'CVaR': Conditional Value at Risk.\n",
    "# 'EVaR': Entropic Value at Risk.\n",
    "# 'WR': Worst Realization (Minimax)\n",
    "# 'MDD': Maximum Drawdown of uncompounded cumulative returns (Calmar Ratio).\n",
    "# 'ADD': Average Drawdown of uncompounded cumulative returns.\n",
    "# 'CDaR': Conditional Drawdown at Risk of uncompounded cumulative returns.\n",
    "# 'EDaR': Entropic Drawdown at Risk of uncompounded cumulative returns.\n",
    "# 'UCI': Ulcer Index of uncompounded cumulative returns.\n",
    "\n",
    "rms = ['MV', 'MAD', 'MSV', 'FLPM', 'SLPM', 'CVaR',\n",
    "       'EVaR', 'WR', 'MDD', 'ADD', 'CDaR', 'UCI', 'EDaR']\n",
    "\n",
    "w_s = pd.DataFrame([])\n",
    "port.alpha = 0.05\n",
    "\n",
    "for i in rms:\n",
    "    if i == 'MV':\n",
    "        hist = False\n",
    "    else:\n",
    "        hist = True\n",
    "    w = port.optimization(model=model, rm=i, obj=obj, rf=rf, l=l, hist=hist)\n",
    "    w_s = pd.concat([w_s, w], axis=1)\n",
    "    \n",
    "w_s.columns = rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_s.style.format(\"{:.2%}\").background_gradient(cmap='YlGn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a comparison of assets weights for each portfolio\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_figwidth(14)\n",
    "fig.set_figheight(6)\n",
    "ax = fig.subplots(nrows=1, ncols=1)\n",
    "\n",
    "w_s.plot.bar(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
